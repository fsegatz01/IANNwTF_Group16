# -*- coding: utf-8 -*-
"""hw06-changes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZHuYL5OEV-BQFXS4rlbci0eFKN4k8ZkX
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

cifar10 = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)

def preprocess_data(cifar10):
  #convert data from uint8 to float32
  cifar10 = cifar10.map(lambda img, target: (tf.cast(img, tf.float32), target))
  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]
  cifar10 = cifar10.map(lambda img, target: ((img/128.)-1., target))
  #create one-hot targets
  cifar10 = cifar10.map(lambda img, target: (img, tf.one_hot(target, depth=10)))
  #cache this progress in memory, as there is no need to redo it; it is deterministic after all
  cifar10 = cifar10.cache()
  #shuffle, batch, prefetch
  cifar10 = cifar10.shuffle(1000)
  cifar10 = cifar10.batch(32) # change batch size
  cifar10 = cifar10.prefetch(20)
  #return preprocessed dataset
  return cifar10

'''
visualizing some example images of our data
'''

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

x_train, x_test = x_train / 255.0, x_test / 255.0 # Reduce pixel values
 
y_train, y_test = y_train.flatten(), y_test.flatten() # flatten the label values

# these are the labels for the cifar10 data set, so 0 means airplane, 1 means automobile and so on
labels = '''airplane automobile bird cat deer dog frog horse ship truck'''.split()

fig, axes = plt.subplots(3,3, figsize=(12,12))
fig.tight_layout()

# showing the images with label
for i, ax in enumerate(axes.flatten()):
    ax.imshow(x_train[i].reshape(32,32,3))
    ax.set_title(labels[y_train[i]])
    ax.axis("off")

#from tensorflow.python.framework.importer import import_graph_def_for_function
class BasicConv(tf.keras.Model):
    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=False): # penalties, dropout, batch normalization

        """ 
        subclass of the tf.keras.Model class, creates metrics

        Args:
            L2_reg(float) = regularizer that applies a L2 regularization penalty
            dropout_rate(float) = sets input units to 0 with a frequency dropout_rate
            batch_norm(bool): using tf.keras.layers.BatchNormalization() or None
        """  
        super(BasicConv, self).__init__()

        kernel_regularizer = tf.keras.regularizers.L2(L2_reg) if L2_reg else None # Wer das liest ist bl√∂d

        self.dropout_rate = dropout_rate
        if self.dropout_rate: # include dropout_rate
           self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)
        
        # creating a list of layers
        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                           tf.keras.layers.Conv2D(32, 3,activation='relu',  kernel_regularizer=kernel_regularizer),
                           tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),
                           tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                           tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                           tf.keras.layers.GlobalAvgPool2D(),
                           tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=kernel_regularizer)]

                           

        if batch_norm: # if batch_norm, add tf.keras.layers.BatchNormalization() layers
          self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.Conv2D(32, 3,activation='relu',  kernel_regularizer=kernel_regularizer),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.GlobalAvgPool2D(),
                              tf.keras.layers.BatchNormalization(),
                              tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=kernel_regularizer)]

        # old model layers
        '''
        self.convlayer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),
        self.convlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),
        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)

        self.convlayer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))
        self.convlayer4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))
        self.global_pool = tf.keras.layers.GlobalAvgPool2D()

        self.out = tf.keras.layers.Dense(10, activation='softmax')
        '''

        # optimizer, loss function and metrics
        self.metrics_list = [tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.CategoricalAccuracy(name="acc")]
        

        self.loss_function = tf.keras.losses.CategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.005) # optimizer Adam

    def call(self, x, training=False):

        """
        activates the net and feeds information forward through layers
        also calculates loss and adjusts weights
        Args:
            x(tf.tensor): data for NN, input images with corresponding targets
            training (boolean) : indicating whether the layer should behave in training mode (adding dropout) or in inference mode (doing nothing)      
        Returns: output from NN                           
        """
        for layer in self.layer_list[:-1]:
                x = layer(x)
                if self.dropout_rate:
                    x = self.dropout_layer(x, training)
        return self.layer_list[-1](x)

    
    # 3. metrics property
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model

    # 4. reset all metrics objects
    def reset_metrics(self):
        """
        return a list with all metrics in the model
        """
        for metric in self.metrics:
            metric.reset_states()

    # added to get frobenius graph
    def compute_frobenius(self):
        """compute and return frobenius norm

        Returns:
            (tensor): frobenius norm
        """
        frobenius_norm = tf.zeros((1,))
        for var in self.trainable_variables:
            frobenius_norm += tf.norm(var, ord="euclidean")
        return frobenius_norm

    # 5. train step method
    def train_step(self, data):
        """
        training the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        
        with tf.GradientTape() as tape:
            output = self(img, training=True)
            label = tf.reshape(label, output.shape)
            #loss = self.loss_function(label, output)
            loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}


    # 6. test_step method
    def test_step(self, data):

        """
        testing the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        output = self(img, training=False)
        label = tf.reshape(label, output.shape)
        #loss = self.loss_function(label, output)
        loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)

        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)
        #print({m.name: m.result() for m in self.metrics})

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}

# TASK 4 - Training the networks

def training(train_losses, test_losses, test_accuracies, train_accuracies):

  """
  trains a neural net for 15 epochs

  Args:
    train_losses(numpy.ndarray):  where we save train_losses for visualization
    test_losses(numpy.ndarray): save test_losses for visualization
    test_accuracies(numpy.ndarray): save train_accuracies for visualization
    train_accuracies(numpy.ndarray): save test_accuracies for visualization

  """
  
  # instantiate the model
  mymodel = BasicConv()
 
  # create datasets
  train_ds = cifar10[0]
  test_ds = cifar10[1]

  train_ds = preprocess_data(train_ds) #train_ds.apply(preprocess)
  test_ds = preprocess_data(test_ds) #val_ds.apply(preprocess)

  # test once in the beginning to check accuracy before training
  for data in test_ds:
    metrics = mymodel.test_step(data)
          
  print("without training: ")
  print([f"test_{key}: {value.numpy()}" for (key, value) in metrics.items()], "\n")
    
  # reset all metrics
  mymodel.reset_metrics()  


  # internal training loop function
  def training_loop(model, train_ds, test_ds, epochs): 

    """
    inner training function, trains a neural net for n epochs

    Args:
      model(tf.keras.Model): model we want to use
      epochs(int): number of epochs we want to train the model
      train_ds(tf.tensor): preprocessed training data
      train_ds(tf.tensor): preprocessed testing data
    """


    for epoch in range(epochs):
        print(f"Epoch {epoch}:")
   

        # Training:
        for data in train_ds:
            metrics = model.train_step(data)
            
        # print the metrics
        print([f"train_{key}: {value.numpy()}" for (key, value) in metrics.items()])

        for key, value in metrics.items():
            if key == "loss":
              train_losses.append(value.numpy())
            elif key == "acc":
              train_accuracies.append(value.numpy())

        # reset all metrics
        model.reset_metrics()
        

        # Testing:
        for data in test_ds:
            metrics = model.test_step(data)
       
        print([f"test_{key}: {value.numpy()}" for (key, value) in metrics.items()])

        for key, value in metrics.items():
          if key == "loss":
            test_losses.append(value.numpy())
          elif key == "acc":
            test_accuracies.append(value.numpy())


        # reset all metrics
        model.reset_metrics() 
        print("\n")  

  
  training_loop(mymodel, train_ds, test_ds, 15) #epochs=15
  return train_losses, test_losses, test_accuracies, train_accuracies

train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

train_losses, test_losses, test_accuracies, train_accuracies = training(train_losses, test_losses, test_accuracies, train_accuracies)

def vizualize(train_losses, test_losses, test_accuracies, train_accuracies, title):
    
    """ Visualizes accuracy and loss for training and test data using the mean of each epoch.
    Loss is displayed in a regular line, accuracy in a dotted line.
    Training data is displayed in blue, test data in red.

    Parameters
    ----------
    train_losses(numpy.ndarray): training losses
    train_accuracies(numpy.ndarray): training accuracies
    test_losses(numpy.ndarray): test losses
    test_accuracies(numpy.ndarray): test_accuracies
    title(string): title of the plot
    """
  
    plt.figure()
    line1, = plt.plot(train_losses)
    line2, = plt.plot(test_losses)
    line3, = plt.plot(test_accuracies)
    line4, = plt.plot(train_accuracies)
    plt.title(title)
    plt.xlabel("Training steps")
    plt.ylabel("Loss/Accuracy")
    plt.legend((line1, line2, line3, line4), ("training loss", "test loss", "test accuracy", "train accuracy"))
    plt.show()

vizualize(train_losses, test_losses, test_accuracies, train_accuracies, "Original")

# %tensorboard --logdir logs/