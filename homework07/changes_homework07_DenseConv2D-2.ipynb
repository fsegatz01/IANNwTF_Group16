{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Nj2xAS0eBO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaPCaVjqF4Bp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dvNcOcC-fWE"
      },
      "outputs": [],
      "source": [
        "mnist = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "''' shape should be (batch_size, sequence_length, features) '''\n",
        "def preprocess_data(mnist, batch_size, sequence_length):\n",
        "  #convert data from uint8 to float32\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "  \n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  mnist = mnist.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  mnist = mnist.shuffle(1000)\n",
        "\n",
        "  #create tensor with length of data/representing indices\n",
        "  stop = sequence_length\n",
        "  sequence_length = tf.range(len(mnist)) \n",
        "  #this creates the alternating positve and negative signes by checkign whether the entry index modulo 2 is zero\n",
        "  #for even index take elem else take -elem\n",
        "  alternating_target_numbers = tf.where(tf.math.floormod(sequence_length, 2)==0, [elem[1] for elem in mnist], [-(elem[1]) for elem in mnist])\n",
        "  #print(\"alternatiing:\", alternating_target_numbers)\n",
        "  c_sum = tf.math.cumsum(alternating_target_numbers)\n",
        "\n",
        "  # get new targets in a vetcor\n",
        "  c_sum = tf.data.Dataset.from_tensor_slices(c_sum)\n",
        "\n",
        "  # put MNIST and new targets together\n",
        "  prepared = tf.data.Dataset.zip((mnist,c_sum))\n",
        "  prepared = prepared.map(lambda img, target: (img[0], target))\n",
        "  #print('prepared', prepared.shape)\n",
        "  #print(\"c_sum:\", c_sum)\n",
        "  mnist = mnist.batch(batch_size)\n",
        "  mnist = mnist.prefetch(20)\n",
        "  \n",
        "  #return preprocessed dataset\n",
        "  return mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgYbjbB0eBz"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(tf.keras.layers.AbstractRNNCell):\n",
        "\n",
        "    def __init__(self, num_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_units = num_units\n",
        "        self.hidden_state = num_units\n",
        "        self.cell_state = num_units\n",
        "        self.states = [self.hidden_state, self.cell_state]\n",
        "        \n",
        "        # first sigmoid layer: forget_gate\n",
        "        self.layer_sigmoid1 = tf.keras.layers.Dense(num_units,\n",
        "                                                     kernel_initializer= tf.keras.initializers.Orthogonal(gain=1.0, seed=None),\n",
        "                                                     activation=tf.nn.sigmoid)\n",
        "        # second sigmoid layer: input_gate\n",
        "        self.layer_sigmoid2 = tf.keras.layers.Dense(num_units, kernel_initializer= tf.keras.initializers.Orthogonal(gain=1.0, seed=None), \n",
        "                                                       activation=tf.nn.sigmoid)    \n",
        "\n",
        "        # tanh layer: input_gate (candidates)\n",
        "        self.layer_tanh = tf.keras.layers.Dense(num_units, kernel_initializer= tf.keras.initializers.Orthogonal(gain=1.0, seed=None), \n",
        "                                                       activation=tf.nn.tanh)\n",
        "        # third sigmoid layer: output_gate\n",
        "        self.layer_sigmoid3 = tf.keras.layers.Dense(num_units, kernel_initializer= tf.keras.initializers.Orthogonal(gain=1.0, seed=None), \n",
        "                                                       activation=tf.nn.sigmoid)\n",
        "        \n",
        "\n",
        "        # layer normalization for trainability\n",
        "        self.layer_norm_h = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_c = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return [tf.TensorShape(self.hidden_state), tf.TensorShape(self.cell_state)]\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return tf.TensorShape(self.hidden_state) # return [tf.TensorShape([self.recurrent_units_2])]\n",
        "\n",
        "\n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None): \n",
        "        return (tf.zeros((32, self.hidden_state,self.hidden_state, 1)),\n",
        "                tf.zeros((32, self.cell_state, self.cell_state, 1)))\n",
        "        \n",
        "\n",
        "    def call(self, input, states):\n",
        "    \n",
        "      self.cell_state, self.hidden_state = states\n",
        "      drive = tf.concat((input, self.hidden_state), axis=1)\n",
        "\n",
        "      forget_gate_drive = self.forget_gate(drive)\n",
        "      forget_drive = forget_gate_drive * self.cell_state\n",
        "\n",
        "      input_gate_drive = self.input_gate(drive)\n",
        "      candiate_gate_drive = self.candidate_gate(drive)\n",
        "      input_candidate_drive = input_gate_drive * candiate_gate_drive\n",
        "\n",
        "      self.cell_state = forget_drive + input_candidate_drive\n",
        "\n",
        "      self.hidden_state = self.output_gate(drive) * self.output_tanh(self.cell_state)\n",
        "\n",
        "      return self.cell_state, self.hidden_state\n",
        "\n",
        "       \n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"hidden state\": self.hidden_state, \n",
        "                \"cell state\": self.cell_state,\n",
        "                \"number of units\": self.num_units}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Layer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, cell):\n",
        "    super().__init__()\n",
        "    self.lstm_cell = cell\n",
        "\n",
        "  def call(self, input):\n",
        "    batch_size = input.shape[0]\n",
        "    sequence_length = input.shape[1]\n",
        "    time_steps = [self.zero_states(batch_size, sequence_length),]\n",
        "    \n",
        "    for i in range(sequence_length):\n",
        "      cell_state, hidden_state = self.lstm_cell(input[:, i, :], time_steps[-1])\n",
        "      time_steps.append((cell_state, hidden_state))\n",
        "    \n",
        "    last_cell_state, last_hidden_state = time_steps[-1]\n",
        "\n",
        "    return last_hidden_state\n",
        "      \n",
        "\n",
        "  def zero_states(self, batch_size, sequence_length):\n",
        "    self.lstm_cell.cell_state = tf.zeros((batch_size, sequence_length), dtype=tf.dtypes.float32)\n",
        "    self.lstm_cell.hidden_state = tf.zeros((batch_size, sequence_length), dtype=tf.dtypes.float32)\n",
        "    \n",
        "    return tf.zeros((batch_size, sequence_length), dtype=tf.dtypes.float32), tf.zeros((batch_size, sequence_length), dtype=tf.dtypes.float32)"
      ],
      "metadata": {
        "id": "flORJ-AGA8X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIw20OVj0eB1"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convlayer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.convlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.time1 = tf.keras.layers.TimeDistributed(self.pooling)\n",
        "\n",
        "        self.convlayer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.convlayer4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.time2 = tf.keras.layers.TimeDistributed(self.pooling2)\n",
        "\n",
        "        #print(tf.shape(self.global_pool))\n",
        "        \n",
        "        \n",
        "        self.lstm_layer = LSTM_Layer(LSTMCell(num_units = 28, dtype=tf.float32))# SHAPE AFTER POOLING = 1x?\n",
        "        \n",
        "        # return_sequences collects and returns the output of the lstm_cell for all time-steps\n",
        "        # unroll unrolls the network for speed (at the cost of memory)\n",
        "        #self.wrap = tf.keras.layers.RNN(self.lstm_layer, return_sequences=True, unroll=True)\n",
        "\n",
        "        self.global_pool = tf.keras.layers.GlobalAvgPool3D() # 3D POOLING BECAUSE SHAPE (32,20,28,28,32)\n",
        "        self.time3 = tf.keras.layers.TimeDistributed(self.global_pool)\n",
        "\n",
        "        \n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation=\"relu\")\n",
        "        \n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                             tf.keras.metrics.BinaryAccuracy()]\n",
        "    \n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_state()\n",
        "        \n",
        "    def call(self, x, training=False):\n",
        "\n",
        "      x = self.convlayer1(x)\n",
        "      print('shape', x.shape)\n",
        "      \n",
        "      x = self.convlayer2(x)\n",
        "      print('shape', x.shape)\n",
        "      \n",
        "      x = self.pooling(x)\n",
        "      print('shape', x.shape)\n",
        "      \n",
        "      x = self.time1(x)\n",
        "      print('shape', x.shape)\n",
        "      \n",
        "      x = self.convlayer3(x)\n",
        "      x = self.convlayer4(x)\n",
        "      x = self.pooling2(x)\n",
        "      x = self.time2(x)\n",
        "      print('shappe before', x.shape)\n",
        "      x = self.lstm_layer(x)\n",
        "      #x = self.wrap(x)\n",
        "      x = self.global_pool(x)\n",
        "      x = self.time3(x)\n",
        "      print('shappe after', x.shape)\n",
        "      \n",
        "      return self.output_layer(x)\n",
        "    \n",
        "    def train_step(self, data):\n",
        "        \n",
        "        \"\"\"\n",
        "        Standard train_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "        \"\"\"\n",
        "        \n",
        "        sequence, label = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(sequence, training=True)\n",
        "            loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        \n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "    \n",
        "    def test_step(self, data):\n",
        "        \n",
        "        \"\"\"\n",
        "        Standard test_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "        \"\"\"\n",
        "        \n",
        "        sequence, label = data\n",
        "        #print(sequence)\n",
        "        output = self(sequence, training=False)\n",
        "        loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "                \n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "JUQZvkdINPFL",
        "outputId": "4c5e0464-2a99-44bf-d6ab-c144d6927302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Loop 1\n",
            "Loop 2\n",
            "shape (32, 28, 28, 32)\n",
            "shape (32, 28, 28, 32)\n",
            "shape (32, 14, 14, 32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-fdae58cd237d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-fdae58cd237d>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#    model.save_weights(save_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mourmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, f\"logs/{str(opt)}/weights\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-fdae58cd237d>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, test_ds, epochs, save_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loop 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m            \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"test_{key}: {value.numpy()}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-57794279c647>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#print(sequence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-57794279c647>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"time_distributed_47\" (type TimeDistributed).\n\nInput 0 of layer \"max_pooling2d_17\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (448, 14, 32)\n\nCall arguments received by layer \"time_distributed_47\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(32, 14, 14, 32), dtype=float32)\n  • training=False\n  • mask=None"
          ]
        }
      ],
      "source": [
        "# TASK 4 - Training the networks\n",
        "\n",
        "\n",
        "def training():\n",
        "  \n",
        "  # instantiate the model\n",
        "  ourmodel = LSTMModel()\n",
        "\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "  # compile the model (here, adding a loss function and an optimizer)\n",
        "  ourmodel.compile(optimizer = optimizer, loss=loss)\n",
        "  \n",
        "  # create datasets\n",
        "  train_ds = mnist[0]\n",
        "  test_ds = mnist[1]\n",
        "\n",
        "  train_ds = preprocess_data(train_ds, batch_size=32, sequence_length =20) #train_ds.apply(preprocess)\n",
        "  test_ds = preprocess_data(test_ds, batch_size=32, sequence_length =20) #val_ds.apply(preprocess)\n",
        "\n",
        "  # internal training loop function\n",
        "  def training_loop(model, train_ds, test_ds, epochs, save_path=False): \n",
        "\n",
        "    #save_path = save_path\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print('Loop 1')\n",
        "        # Validation:\n",
        "        # (we do the validation first so that we get the accuracy and loss before training the network)\n",
        "        for data in test_ds:\n",
        "           print('Loop 2')\n",
        "           metrics = model.test_step(data)\n",
        "        \n",
        "        print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()    \n",
        "        \n",
        "        # Training:\n",
        "        for data in train_ds:\n",
        "           print('Loop 1')\n",
        "           metrics = model.train_step(data)\n",
        "\n",
        "        # print the metrics\n",
        "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        print(\"\\n\")\n",
        "\n",
        "    #save weights\n",
        "    #if save_path:\n",
        "    #    model.save_weights(save_path)\n",
        "  \n",
        "  training_loop(ourmodel, train_ds, test_ds, 11)#, f\"logs/{str(opt)}/weights\")\n",
        "  print('end')\n",
        "  return\n",
        "\n",
        "training()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxjx-9Ds0eB2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtpF0a-I0eB5"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"lstm_noise\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{EXPERIMENT_NAME}/{current_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOB6ZtU30eB6"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    initial_epoch=25,\n",
        "                    epochs=50,\n",
        "                    callbacks=[logging_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdJ2Kfle0eB7"
      },
      "outputs": [],
      "source": [
        "# save the complete model (incl. optimizer state, loss function, metrics etc.)\n",
        "# ideally save to google drive if you're using colab\n",
        "model.save(\"saved_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-ocZHpm0eB8"
      },
      "outputs": [],
      "source": [
        "# load the model and resume training where we had to stop\n",
        "loaded_model = tf.keras.models.load_model(\"saved_model\", custom_objects={\"LSTMCell\": LSTMCell,\n",
        "                                                                         \"LSTMModel\": LSTMModel})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}