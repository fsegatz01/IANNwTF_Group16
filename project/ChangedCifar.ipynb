{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g5ldTCY_HV11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96820fa5-23d1-41b8-850d-a8d30935712e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nilearn==0.9.2 in /usr/local/lib/python3.9/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (1.10.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (4.9.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (1.1.1)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (2.27.1)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (3.0.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.9.2) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.9.2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.9.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.9.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.9.2) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->nilearn==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0->nilearn==0.9.2) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn==0.9.2\n",
        "\n",
        "# !pip install keras_cv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from keras import datasets, Sequential\n",
        "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
        "# import keras_cv\n",
        "import keras\n",
        "import datetime\n",
        "\n",
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "xFo3PmgLHoYg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "UW42vfCmHePm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## LOADING CIFAR DATASET #############\n",
        "\n",
        "cifar, cifar_info = tfds.load(\"cifar100\", as_supervised=True, with_info =True)\n",
        "\n",
        "# split data into train and test\n",
        "train_ds = cifar[\"train\"]\n",
        "val_ds = cifar[\"test\"]\n"
      ],
      "metadata": {
        "id": "lK9Tzny4yD30"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "mOUK9-LZHhwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# PREPROCESS FUNCTION #################\n",
        "\n",
        "def data_pip(cifar10, dataset_type, batch_size):\n",
        "\n",
        "  #cifar10 = tf.data.Dataset.from_tensor_slices(cifar10)\n",
        "  cifar10 = cifar10.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  cifar10 = cifar10.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #create one-hot targets\n",
        "  cifar10 = cifar10.map(lambda img, target: (img, tf.one_hot(target, depth=100)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  # cifar10 = cifar10.cache()\n",
        "  # shuffle, batch, prefetch\n",
        "  cifar10 = cifar10.shuffle(1000)\n",
        "  # cifar10 = cifar10.map(lambda img, target: (tf.expand_dims(img, 0), target))\n",
        "  cifar10 = cifar10.batch(64)\n",
        "  cifar10 = cifar10.prefetch(20)\n",
        "  return cifar10"
      ],
      "metadata": {
        "id": "x4F5O5dWJopo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6ccWlYM7K5",
        "outputId": "a2cb61d5-b521-4773-8c31-b723fc6676ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar = data_pip(train_ds, 'cifar', 32)\n",
        "test_cifar = data_pip(val_ds, 'cifar', 32)"
      ],
      "metadata": {
        "id": "VI1MHbswKAwk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### PREPROCESS ################\n",
        "\n",
        "#train_cifar = tf.data.Dataset.from_tensor_slices((train_ds))\n",
        "#test_cifar = tf.data.Dataset.from_tensor_slices((val_ds)\n",
        "\n",
        "'''\n",
        "issa = tf.data.Dataset.from_tensor_slices((train_fab, train_y))\n",
        "\n",
        "cifar_data = data_pip(issa, 'cifar', 32)\n",
        "print(cifar_data) '''"
      ],
      "metadata": {
        "id": "Thr67_8RXujP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "9DfKznMzHuoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############ VISUALIZATION ################\n",
        "\n",
        "# tfds.show_examples(cifar_data, cifar_info)\n",
        "# tfds.show_examples ( train_ds , ds_info )"
      ],
      "metadata": {
        "id": "TmdDuiPPQCQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "\n",
        "[tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')),\n",
        "                           tf.keras.layers.Flatten(),\n",
        "                           tf.keras.layers.Dense(256, activation='relu'),\n",
        "                           tf.keras.layers.Dense(128, activation='relu'),\n",
        "                           tf.keras.layers.Dense(100, activation='softmax')]\n"
      ],
      "metadata": {
        "id": "o80Zkq72RDG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## NEUES MODEL TRAYYYY ##########\n",
        "\n",
        "#from tensorflow.python.framework.importer import import_graph_def_for_function\n",
        "class BasicConv(tf.keras.Model):\n",
        "    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=False): # penalties, dropout, batch normalization\n",
        "\n",
        "        \"\"\" \n",
        "        subclass of the tf.keras.Model class, creates metrics\n",
        "        Args:\n",
        "            L2_reg(float) = regularizer that applies a L2 regularization penalty\n",
        "            dropout_rate(float) = sets input units to 0 with a frequency dropout_rate\n",
        "            batch_norm(bool): using tf.keras.layers.BatchNormalization() or None\n",
        "        \"\"\"  \n",
        "        super(BasicConv, self).__init__()\n",
        "\n",
        "        kernel_regularizer = tf.keras.regularizers.L2(L2_reg) if L2_reg else None # Wer das liest ist blöd\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        if self.dropout_rate: # include dropout_rate\n",
        "           self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "        \n",
        "        # creating a list of layers\n",
        "        self.layer_list = [tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32),\n",
        "                           tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64),\n",
        "                           tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
        "                           tf.keras.layers.Flatten(),\n",
        "                           tf.keras.layers.Dense(256, activation='relu'),\n",
        "                           tf.keras.layers.Dense(128, activation='relu'),\n",
        "                           tf.keras.layers.Dense(100, activation='softmax')]\n",
        "\n",
        "                           \n",
        "\n",
        "        if batch_norm: # if batch_norm, add tf.keras.layers.BatchNormalization() layers\n",
        "          self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(32, 3,activation='relu',  kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.GlobalAvgPool2D(),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Dense(100, activation='softmax', kernel_regularizer=kernel_regularizer)]\n",
        "\n",
        "        # old model layers\n",
        "        '''\n",
        "        self.convlayer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "        self.convlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.convlayer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.convlayer4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "        '''\n",
        "\n",
        "        # optimizer, loss function and metrics\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                             tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
        "        \n",
        "\n",
        "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.005) # optimizer Adam\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "\n",
        "        \"\"\"\n",
        "        activates the net and feeds information forward through layers\n",
        "        also calculates loss and adjusts weights\n",
        "        Args:\n",
        "            x(tf.tensor): data for NN, input images with corresponding targets\n",
        "            training (boolean) : indicating whether the layer should behave in training mode (adding dropout) or in inference mode (doing nothing)      \n",
        "        Returns: output from NN                           \n",
        "        \"\"\"\n",
        "        for layer in self.layer_list[:-1]:\n",
        "                x = layer(x)\n",
        "                if self.dropout_rate:\n",
        "                    x = self.dropout_layer(x, training)\n",
        "        return self.layer_list[-1](x)\n",
        "\n",
        "    \n",
        "    # 3. metrics property\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "        # return a list with all metrics in the model\n",
        "\n",
        "    # 4. reset all metrics objects\n",
        "    def reset_metrics(self):\n",
        "        \"\"\"\n",
        "        return a list with all metrics in the model\n",
        "        \"\"\"\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "    # added to get frobenius graph\n",
        "    def compute_frobenius(self):\n",
        "        \"\"\"compute and return frobenius norm\n",
        "        Returns:\n",
        "            (tensor): frobenius norm\n",
        "        \"\"\"\n",
        "        frobenius_norm = tf.zeros((1,))\n",
        "        for var in self.trainable_variables:\n",
        "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
        "        return frobenius_norm\n",
        "\n",
        "    # 5. train step method\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        training the network for once\n",
        "        Args:\n",
        "            data: input data (image with target)\n",
        "        Returns:\n",
        "            Return a dictionary mapping metric names to current value\n",
        "        \"\"\"\n",
        "\n",
        "        img, label = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(img, training=True)\n",
        "            label = tf.reshape(label, output.shape)\n",
        "            #loss = self.loss_function(label, output)\n",
        "            loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update the state of the metrics according to loss and accuracy\n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "\n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "    # 6. test_step method\n",
        "    def test_step(self, data):\n",
        "\n",
        "        \"\"\"\n",
        "        testing the network for once\n",
        "        Args:\n",
        "            data: input data (image with target)\n",
        "        Returns:\n",
        "            Return a dictionary mapping metric names to current value\n",
        "        \"\"\"\n",
        "\n",
        "        img, label = data\n",
        "        output = self(img, training=False)\n",
        "        label = tf.reshape(label, output.shape)\n",
        "        #loss = self.loss_function(label, output)\n",
        "        loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)\n",
        "\n",
        "        # update the state of the metrics according to loss and accuracy\n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "        #print({m.name: m.result() for m in self.metrics})\n",
        "\n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "CyO5Z_eqSfb8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "4DphxkMqH3vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ShYeapWuN9qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.python.framework.importer import import_graph_def_for_function\n",
        "class BasicConv(tf.keras.Model):\n",
        "    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=False): # penalties, dropout, batch normalization\n",
        "\n",
        "        \"\"\" \n",
        "        subclass of the tf.keras.Model class, creates metrics\n",
        "        Args:\n",
        "            L2_reg(float) = regularizer that applies a L2 regularization penalty\n",
        "            dropout_rate(float) = sets input units to 0 with a frequency dropout_rate\n",
        "            batch_norm(bool): using tf.keras.layers.BatchNormalization() or None\n",
        "        \"\"\"  \n",
        "        super(BasicConv, self).__init__()\n",
        "\n",
        "        kernel_regularizer = tf.keras.regularizers.L2(L2_reg) if L2_reg else None # Wer das liest ist blöd\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        if self.dropout_rate: # include dropout_rate\n",
        "           self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "        \n",
        "        # creating a list of layers\n",
        "        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                           tf.keras.layers.Conv2D(32, 3,activation='relu',  kernel_regularizer=kernel_regularizer),\n",
        "                           tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                           tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                           tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                           tf.keras.layers.GlobalAvgPool2D(),\n",
        "                           tf.keras.layers.Dense(100, activation='softmax', kernel_regularizer=kernel_regularizer)]\n",
        "\n",
        "                           \n",
        "\n",
        "        if batch_norm: # if batch_norm, add tf.keras.layers.BatchNormalization() layers\n",
        "          self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(32, 3,activation='relu',  kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=kernel_regularizer),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.GlobalAvgPool2D(),\n",
        "                              tf.keras.layers.BatchNormalization(),\n",
        "                              tf.keras.layers.Dense(100, activation='softmax', kernel_regularizer=kernel_regularizer)]\n",
        "\n",
        "        # old model layers\n",
        "        '''\n",
        "        self.convlayer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "        self.convlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.convlayer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.convlayer4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "        '''\n",
        "\n",
        "        # optimizer, loss function and metrics\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                             tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
        "        \n",
        "\n",
        "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.005) # optimizer Adam\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "\n",
        "        \"\"\"\n",
        "        activates the net and feeds information forward through layers\n",
        "        also calculates loss and adjusts weights\n",
        "        Args:\n",
        "            x(tf.tensor): data for NN, input images with corresponding targets\n",
        "            training (boolean) : indicating whether the layer should behave in training mode (adding dropout) or in inference mode (doing nothing)      \n",
        "        Returns: output from NN                           \n",
        "        \"\"\"\n",
        "        for layer in self.layer_list[:-1]:\n",
        "                x = layer(x)\n",
        "                if self.dropout_rate:\n",
        "                    x = self.dropout_layer(x, training)\n",
        "        return self.layer_list[-1](x)\n",
        "\n",
        "    \n",
        "    # 3. metrics property\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "        # return a list with all metrics in the model\n",
        "\n",
        "    # 4. reset all metrics objects\n",
        "    def reset_metrics(self):\n",
        "        \"\"\"\n",
        "        return a list with all metrics in the model\n",
        "        \"\"\"\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "    # added to get frobenius graph\n",
        "    def compute_frobenius(self):\n",
        "        \"\"\"compute and return frobenius norm\n",
        "        Returns:\n",
        "            (tensor): frobenius norm\n",
        "        \"\"\"\n",
        "        frobenius_norm = tf.zeros((1,))\n",
        "        for var in self.trainable_variables:\n",
        "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
        "        return frobenius_norm\n",
        "\n",
        "    # 5. train step method\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        training the network for once\n",
        "        Args:\n",
        "            data: input data (image with target)\n",
        "        Returns:\n",
        "            Return a dictionary mapping metric names to current value\n",
        "        \"\"\"\n",
        "\n",
        "        img, label = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(img, training=True)\n",
        "            label = tf.reshape(label, output.shape)\n",
        "            #loss = self.loss_function(label, output)\n",
        "            loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update the state of the metrics according to loss and accuracy\n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "\n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "    # 6. test_step method\n",
        "    def test_step(self, data):\n",
        "\n",
        "        \"\"\"\n",
        "        testing the network for once\n",
        "        Args:\n",
        "            data: input data (image with target)\n",
        "        Returns:\n",
        "            Return a dictionary mapping metric names to current value\n",
        "        \"\"\"\n",
        "\n",
        "        img, label = data\n",
        "        output = self(img, training=False)\n",
        "        label = tf.reshape(label, output.shape)\n",
        "        #loss = self.loss_function(label, output)\n",
        "        loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)\n",
        "\n",
        "        # update the state of the metrics according to loss and accuracy\n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(label, output)\n",
        "        #print({m.name: m.result() for m in self.metrics})\n",
        "\n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "Qa4jGGh4KdOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRaining"
      ],
      "metadata": {
        "id": "VVInI5q_KlYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = BasicConv()"
      ],
      "metadata": {
        "id": "N6H4CV-hVm1t"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 - Training the networks\n",
        "\n",
        "def training(mymodel,train_cifar, test_cifar, train_losses, test_losses, test_accuracies, train_accuracies):\n",
        "\n",
        "  \"\"\"\n",
        "  trains a neural net for 15 epochs\n",
        "  Args:\n",
        "    train_losses(numpy.ndarray):  where we save train_losses for visualization\n",
        "    test_losses(numpy.ndarray): save test_losses for visualization\n",
        "    test_accuracies(numpy.ndarray): save train_accuracies for visualization\n",
        "    train_accuracies(numpy.ndarray): save test_accuracies for visualization\n",
        "  \"\"\"\n",
        "  \n",
        "  # instantiate the model\n",
        "  #mymodel = BasicConv()\n",
        "\n",
        "  # test once in the beginning to check accuracy before training\n",
        "  for data in test_cifar:\n",
        "    metrics = mymodel.test_step(data)\n",
        "          \n",
        "  print(\"without training: \")\n",
        "  print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()], \"\\n\")\n",
        "    \n",
        "  # reset all metrics\n",
        "  mymodel.reset_metrics()  \n",
        "\n",
        "\n",
        "  # internal training loop function\n",
        "  def training_loop(model, train_cifar, test_cifar, epochs): \n",
        "\n",
        "    \"\"\"\n",
        "    inner training function, trains a neural net for n epochs\n",
        "    Args:\n",
        "      model(tf.keras.Model): model we want to use\n",
        "      epochs(int): number of epochs we want to train the model\n",
        "      train_ds(tf.tensor): preprocessed training data\n",
        "      train_ds(tf.tensor): preprocessed testing data\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "   \n",
        "\n",
        "        # Training:\n",
        "        for data in train_cifar:\n",
        "            metrics = model.train_step(data)\n",
        "            \n",
        "        # print the metrics\n",
        "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        for key, value in metrics.items():\n",
        "            if key == \"loss\":\n",
        "              train_losses.append(value.numpy())\n",
        "            elif key == \"acc\":\n",
        "              train_accuracies.append(value.numpy())\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        \n",
        "\n",
        "        # Testing:\n",
        "        for data in test_cifar:\n",
        "            metrics = model.test_step(data)\n",
        "       \n",
        "        print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        for key, value in metrics.items():\n",
        "          if key == \"loss\":\n",
        "            test_losses.append(value.numpy())\n",
        "          elif key == \"acc\":\n",
        "            test_accuracies.append(value.numpy())\n",
        "\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics() \n",
        "        print(\"\\n\")  \n",
        "\n",
        "  \n",
        "  training_loop(mymodel, train_cifar, test_cifar, 30) #epochs=15\n",
        "  return train_losses, test_losses, test_accuracies, train_accuracies\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "train_losses, test_losses, test_accuracies, train_accuracies = training(mymodel,train_cifar, test_cifar, train_losses, test_losses, test_accuracies, train_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1nBOJhRKjSo",
        "outputId": "68c52a49-fbaa-4508-eeef-9afa3dc9bd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without training: \n",
            "['test_loss: 4.60564661026001', 'test_acc: 0.009700000286102295'] \n",
            "\n",
            "Epoch 0:\n",
            "['train_loss: 4.603296279907227', 'train_acc: 0.012140000239014626']\n",
            "['test_loss: 4.600547790527344', 'test_acc: 0.011500000022351742']\n",
            "\n",
            "\n",
            "Epoch 1:\n",
            "['train_loss: 4.597024917602539', 'train_acc: 0.018239999189972878']\n",
            "['test_loss: 4.592490196228027', 'test_acc: 0.01810000091791153']\n",
            "\n",
            "\n",
            "Epoch 2:\n",
            "['train_loss: 4.583359241485596', 'train_acc: 0.024299999698996544']\n",
            "['test_loss: 4.570327281951904', 'test_acc: 0.02329999953508377']\n",
            "\n",
            "\n",
            "Epoch 3:\n",
            "['train_loss: 4.546092510223389', 'train_acc: 0.024739999324083328']\n",
            "['test_loss: 4.5185627937316895', 'test_acc: 0.02969999983906746']\n",
            "\n",
            "\n",
            "Epoch 4:\n",
            "['train_loss: 4.46338415145874', 'train_acc: 0.041039999574422836']\n",
            "['test_loss: 4.384319305419922', 'test_acc: 0.05119999870657921']\n",
            "\n",
            "\n",
            "Epoch 5:\n",
            "['train_loss: 4.2458577156066895', 'train_acc: 0.06313999742269516']\n",
            "['test_loss: 4.134215354919434', 'test_acc: 0.06970000267028809']\n",
            "\n",
            "\n",
            "Epoch 6:\n",
            "['train_loss: 4.041088581085205', 'train_acc: 0.08383999764919281']\n",
            "['test_loss: 3.98797869682312', 'test_acc: 0.09470000118017197']\n",
            "\n",
            "\n",
            "Epoch 7:\n",
            "['train_loss: 3.9153151512145996', 'train_acc: 0.10363999754190445']\n",
            "['test_loss: 3.891638994216919', 'test_acc: 0.1080000028014183']\n",
            "\n",
            "\n",
            "Epoch 8:\n",
            "['train_loss: 3.8211214542388916', 'train_acc: 0.12067999690771103']\n",
            "['test_loss: 3.815538167953491', 'test_acc: 0.12049999833106995']\n",
            "\n",
            "\n",
            "Epoch 9:\n",
            "['train_loss: 3.743523120880127', 'train_acc: 0.13242000341415405']\n",
            "['test_loss: 3.7317893505096436', 'test_acc: 0.13500000536441803']\n",
            "\n",
            "\n",
            "Epoch 10:\n",
            "['train_loss: 3.6798062324523926', 'train_acc: 0.14283999800682068']\n",
            "['test_loss: 3.6921770572662354', 'test_acc: 0.14480000734329224']\n",
            "\n",
            "\n",
            "Epoch 11:\n",
            "['train_loss: 3.6264102458953857', 'train_acc: 0.150859996676445']\n",
            "['test_loss: 3.656481981277466', 'test_acc: 0.14650000631809235']\n",
            "\n",
            "\n",
            "Epoch 12:\n",
            "['train_loss: 3.580630302429199', 'train_acc: 0.15838000178337097']\n",
            "['test_loss: 3.605050563812256', 'test_acc: 0.1525000035762787']\n",
            "\n",
            "\n",
            "Epoch 13:\n",
            "['train_loss: 3.5365254878997803', 'train_acc: 0.16694000363349915']\n",
            "['test_loss: 3.574956178665161', 'test_acc: 0.16019999980926514']\n",
            "\n",
            "\n",
            "Epoch 14:\n",
            "['train_loss: 3.4926886558532715', 'train_acc: 0.17404000461101532']\n",
            "['test_loss: 3.5339467525482178', 'test_acc: 0.16750000417232513']\n",
            "\n",
            "\n",
            "Epoch 15:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############# NO OPTIMIzations preventing overiftting ####################\n",
        "\n",
        "def create_CNN_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "dyU7FYccvMTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## CREATE MODEL ###########\n",
        "\n",
        "cnnn_model = create_CNN_model()\n",
        "\n",
        "cnnn_model.summary()"
      ],
      "metadata": {
        "id": "g9Arqc3YZZH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50 Model"
      ],
      "metadata": {
        "id": "CCyZ4MauH-TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## ResNet50 MODEL  #############\n",
        "\n",
        "resnet_model = tf.keras.applications.resnet.ResNet50(\n",
        "       input_shape=(32,32,3), #.output_shapes()[0],\n",
        "       include_top=True,\n",
        "       weights=None)\n",
        "\n",
        "resnet_model.compile(\n",
        "       optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #optimizer=tf.keras.optimizers.SGD(lr=0.001),\n",
        "      loss='sparse_categorical_crossentropy', #loss='binary_crossentropy',\n",
        "       metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "XxgC9GMeQkm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Test Model\n",
        "weights and tensorboard logging"
      ],
      "metadata": {
        "id": "QfH-1861JCVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set tensorboard \n",
        "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/CNN_model/{current_time}\")\n",
        "\n",
        "# Train the model\n",
        "cnnn_model.fit(cifar_data, batch_size=64, epochs=5) #callbacks=[logging_callback]) #batch_size=64, \n",
        "\n",
        "# Test Step\n",
        "test_loss, test_acc = cnnn_model.evaluate(cifar_data)\n",
        "print(\"test accuracy: \", test_acc)\n",
        "\n",
        "# save the weights\n",
        "# cnn_model.save_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# logging with tensorboard\n",
        "# %tensorboard --logdir=\"logs/CNN_model\""
      ],
      "metadata": {
        "id": "JT4hTDqByxmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### LOAD OLD WEIGHTS #########\n",
        "'''\n",
        "# Create a new model instance\n",
        "new_model = create_CNN_model()\n",
        "\n",
        "# Restore the weights\n",
        "new_model.load_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = new_model.evaluate(x_test_cifar, y_test_cifar)\n",
        "print(\"test accuracy: \", acc)'''\n"
      ],
      "metadata": {
        "id": "lO2sMnMLI0e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "kk2kWW_IIM19"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQXVG0NYLZgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}