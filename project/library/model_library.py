# -*- coding: utf-8 -*-
"""model_library

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VA7WCM53jbZvASlxwTg7nqupoXJJhN-
"""

import h5py
import tensorflow_datasets as tfds
import tensorflow as tf
from matplotlib import pyplot as plt
from google.colab import drive
import os
from os.path import exists

### CNN MODEL ###

class ConvModel(tf.keras.Model):
    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=True): # penalties, dropout, batch normalization

        """ 
        subclass of the tf.keras.Model class, creates metrics
        Args:
            L2_reg(float) = regularizer that applies a L2 regularization penalty
            dropout_rate(float) = sets input units to 0 with a frequency dropout_rate
            batch_norm(bool): using tf.keras.layers.BatchNormalization() or None
        """  
        super(ConvModel, self).__init__()

        # overfitting prevention
        kernel_regularizer = tf.keras.regularizers.L2(L2_reg) if L2_reg else None

        self.dropout_rate = dropout_rate
        if self.dropout_rate: # include dropout_rate
           self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)

        # the layers
        self.Conv1 = tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32)
        self.BatchNorm1 = tf.keras.layers.BatchNormalization()
        self.MaxPool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm2 = tf.keras.layers.BatchNormalization()

        self.Conv2 = tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64)
        self.BatchNorm3 = tf.keras.layers.BatchNormalization()
        self.MaxPool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm4 = tf.keras.layers.BatchNormalization()

        self.Conv3 = tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=128)
        self.BatchNorm5 = tf.keras.layers.BatchNormalization()
        self.MaxPool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm6 = tf.keras.layers.BatchNormalization()

        self.flatten = tf.keras.layers.Flatten()
        self.BatchNorm7 = tf.keras.layers.BatchNormalization()
        self.dense1 = tf.keras.layers.Dense(256)
        self.dense2 = tf.keras.layers.Dense(128)
        self.dense3 = tf.keras.layers.Dense(100)
        

        # optimizer, loss function and metrics
        self.metrics_list = [tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.CategoricalAccuracy(name="acc")]
        

        self.loss_function = tf.keras.losses.CategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    def build(self, input_shape):
      self.dense3(self.dense2(self.dense1(self.BatchNorm7(self.flatten(self.BatchNorm6(self.MaxPool3(self.BatchNorm5(self.Conv3(self.BatchNorm4(self.MaxPool2(self.BatchNorm3(self.Conv2(self.BatchNorm2(self.MaxPool1(self.BatchNorm1(self.Conv1(tf.keras.layers.Input(shape=input_shape[1:], name="input_x"))))))))))))))))))

    def call(self, x, training=False):

      x = self.Conv1(x)
      x = self.BatchNorm1(x, training)
      x = self.MaxPool1(x)
      x = self.BatchNorm2(x, training)

      x = self.Conv2(x)
      x = self.BatchNorm3(x, training)
      x = self.MaxPool2(x)
      x = self.BatchNorm4(x, training)

      x = self.Conv3(x)
      x = self.BatchNorm5(x, training)
      x = self.MaxPool3(x)
      x = self.BatchNorm6(x, training)

      x = self.flatten(x)
      x = self.BatchNorm7(x, training)
      x = self.dense1(x)
      x = tf.keras.layers.ReLU()(x)
      x = self.dense2(x)
      x = tf.keras.layers.ReLU()(x)
      x = self.dense3(x)
      x = tf.keras.layers.Softmax()(x)

      return x


    
    # 3. metrics property
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model

    # 4. reset all metrics objects
    def reset_metrics(self):
        """
        return a list with all metrics in the model
        """
        for metric in self.metrics:
            metric.reset_states()

    # added to get frobenius graph
    def compute_frobenius(self):
        """compute and return frobenius norm
        Returns:
            (tensor): frobenius norm
        """
        frobenius_norm = tf.zeros((1,))
        for var in self.trainable_variables:
            frobenius_norm += tf.norm(var, ord="euclidean")
        return frobenius_norm

    # 5. train step method
    def train_step(self, data):
        """
        training the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        
        with tf.GradientTape() as tape:
            output = self(img, training=True)
            loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}


    # 6. test_step method
    def test_step(self, data):

        """
        testing the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        output = self(img, training=False)
        loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)

        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}

### PRETRAINED RESNET50 MODEL ###

class ResNetModel(tf.keras.Model):
    def __init__(self):

      super(ResNetModel, self).__init__()

      self.resnet50 = tf.keras.applications.resnet50.ResNet50(input_shape=(32,32,3), include_top=False, weights='imagenet', classes=100)
      self.GlobalAvgPool = tf.keras.layers.GlobalAveragePooling2D()
      self.output_layer = tf.keras.layers.Dense(100, activation='softmax')
        
      self.loss_function = tf.keras.losses.CategoricalCrossentropy()
      self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    
    def call(self, x): # training=False
      x = self.resnet50(x)
      x = self.GlobalAvgPool(x)
      x = self.output_layer(x)
      return  x

### UNTRAINED RESNET50 MODEL ###

class untrainedResNetModel(tf.keras.Model):
    def __init__(self):

      super(untrainedResNetModel, self).__init__()

      self.resnet50 = tf.keras.applications.resnet.ResNet50(input_shape=(32,32,3), include_top=True, weights=None, classes=100) 
      self.GlobalAvgPool = tf.keras.layers.GlobalAveragePooling2D()
      self.output_layer = tf.keras.layers.Dense(100, activation='softmax')

      self.loss_function = tf.keras.losses.CategoricalCrossentropy()
      self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    
    def call(self, x): # training=False
      x = self.resnet50(x)
      x = self.GlobalAvgPool(x)
      x = self.output_layer(x)
      return  x

def vizualize_performance(train_losses, train_accuracies, test_losses, test_accuracies, title):
    
    """ Visualizes accuracy and loss for training and test data using the mean of each epoch.

    Parameters
    ----------
    train_losses(numpy.ndarray): training losses
    train_accuracies(numpy.ndarray): training accuracies
    test_losses(numpy.ndarray): test losses
    test_accuracies(numpy.ndarray): test_accuracies
    title(string): title of the plot
    """
  
    plt.figure()
    line1, = plt.plot(train_losses)
    line2, = plt.plot(test_losses)
    line3, = plt.plot(test_accuracies)
    line4, = plt.plot(train_accuracies)
    plt.title(title)
    plt.xlabel("Training steps")
    plt.ylabel("Loss/Accuracy")
    plt.legend((line1, line2, line3, line4), ("training loss", "test_losses", "test_accuracies", "train accuracy"))
    plt.show()