# -*- coding: utf-8 -*-
"""Models_Library

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VA7WCM53jbZvASlxwTg7nqupoXJJhN-
"""

import h5py
import tensorflow_datasets as tfds
import tensorflow as tf
from matplotlib import pyplot as plt
from google.colab import drive
import os
from os.path import exists

### CNN MODEL ###

class GoodConvModel(tf.keras.Model):
    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=True): # penalties, dropout, batch normalization

        """ 
        subclass of the tf.keras.Model class, creates metrics
        Args:
            L2_reg(float) = regularizer that applies a L2 regularization penalty
            dropout_rate(float) = sets input units to 0 with a frequency dropout_rate
            batch_norm(bool): using tf.keras.layers.BatchNormalization() or None
        """  
        super(GoodConvModel, self).__init__()

        # overfitting prevention
        kernel_regularizer = tf.keras.regularizers.L2(L2_reg) if L2_reg else None

        self.dropout_rate = dropout_rate
        if self.dropout_rate: # include dropout_rate
           self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)

        # the layers
        self.Conv1 = tf.keras.layers.Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32)
        self.BatchNorm1 = tf.keras.layers.BatchNormalization()
        self.MaxPool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm2 = tf.keras.layers.BatchNormalization()

        self.Conv2 = tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64)
        self.BatchNorm3 = tf.keras.layers.BatchNormalization()
        self.MaxPool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm4 = tf.keras.layers.BatchNormalization()

        self.Conv3 = tf.keras.layers.Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=128)
        self.BatchNorm5 = tf.keras.layers.BatchNormalization()
        self.MaxPool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')
        self.BatchNorm6 = tf.keras.layers.BatchNormalization()

        self.flatten = tf.keras.layers.Flatten()
        self.BatchNorm7 = tf.keras.layers.BatchNormalization()
        self.dense1 = tf.keras.layers.Dense(256)
        self.dense2 = tf.keras.layers.Dense(128)
        self.dense3 = tf.keras.layers.Dense(100)
        

        # optimizer, loss function and metrics
        self.metrics_list = [tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.CategoricalAccuracy(name="acc")]
        

        self.loss_function = tf.keras.losses.CategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    def build(self, input_shape):
      self.dense3(self.dense2(self.dense1(self.BatchNorm7(self.flatten(self.BatchNorm6(self.MaxPool3(self.BatchNorm5(self.Conv3(self.BatchNorm4(self.MaxPool2(self.BatchNorm3(self.Conv2(self.BatchNorm2(self.MaxPool1(self.BatchNorm1(self.Conv1(tf.keras.layers.Input(shape=input_shape[1:], name="input_x"))))))))))))))))))

    def call(self, x, training=False):

      x = self.Conv1(x)
      x = self.BatchNorm1(x, training)
      x = self.MaxPool1(x)
      x = self.BatchNorm2(x, training)

      x = self.Conv2(x)
      x = self.BatchNorm3(x, training)
      x = self.MaxPool2(x)
      x = self.BatchNorm4(x, training)

      x = self.Conv3(x)
      x = self.BatchNorm5(x, training)
      x = self.MaxPool3(x)
      x = self.BatchNorm6(x, training)

      x = self.flatten(x)
      x = self.BatchNorm7(x, training)
      x = self.dense1(x)
      x = tf.keras.layers.ReLU()(x)
      x = self.dense2(x)
      x = tf.keras.layers.ReLU()(x)
      x = self.dense3(x)
      x = tf.keras.layers.Softmax()(x)

      return x


    
    # 3. metrics property
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model

    # 4. reset all metrics objects
    def reset_metrics(self):
        """
        return a list with all metrics in the model
        """
        for metric in self.metrics:
            metric.reset_states()

    # added to get frobenius graph
    def compute_frobenius(self):
        """compute and return frobenius norm
        Returns:
            (tensor): frobenius norm
        """
        frobenius_norm = tf.zeros((1,))
        for var in self.trainable_variables:
            frobenius_norm += tf.norm(var, ord="euclidean")
        return frobenius_norm

    # 5. train step method
    def train_step(self, data):
        """
        training the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        
        with tf.GradientTape() as tape:
            output = self(img, training=True)
            loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}


    # 6. test_step method
    def test_step(self, data):

        """
        testing the network for once
        Args:
            data: input data (image with target)
        Returns:
            Return a dictionary mapping metric names to current value
        """

        img, label = data
        output = self(img, training=False)
        loss = self.loss_function(label, output) + tf.reduce_sum(self.losses)

        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}

### PRETRAINED RESNET50 MODEL ###

class ResNetModel(tf.keras.Model):
    def __init__(self):

      super(ResNetModel, self).__init__()

      self.resnet50 = tf.keras.applications.resnet50.ResNet50(input_shape=(32,32,3), include_top=False, weights='imagenet', classes=100)
      self.GlobalAvgPool = tf.keras.layers.GlobalAveragePooling2D()
      self.output_layer = tf.keras.layers.Dense(100, activation='softmax')
        
      self.loss_function = tf.keras.losses.CategoricalCrossentropy()
      self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    
    def call(self, x): # training=False
      x = self.resnet50(x)
      x = self.GlobalAvgPool(x)
      x = self.output_layer(x)
      return  x

### UNTRAINED RESNET50 MODEL ###

class untrainedResNetModel(tf.keras.Model):
    def __init__(self):

      super(untrainedResNetModel, self).__init__()

      self.resnet50 = tf.keras.applications.resnet.ResNet50(input_shape=(32,32,3), include_top=True, weights=None, classes=100) 
      self.GlobalAvgPool = tf.keras.layers.GlobalAveragePooling2D()
      self.output_layer = tf.keras.layers.Dense(100, activation='softmax')

      self.loss_function = tf.keras.losses.CategoricalCrossentropy()
      self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    
    def call(self, x): # training=False
      x = self.resnet50(x)
      x = self.GlobalAvgPool(x)
      x = self.output_layer(x)
      return  x



########## LOADING CIFAR DATASET #############

cifar, cifar_info = tfds.load("cifar100", as_supervised=True, with_info =True)

# split data into train and test
train_ds = cifar["train"]
val_ds = cifar["test"]



drive.mount('/content/drive', force_remount=True)
os.chdir ("drive/MyDrive")

filename = "/content/drive/MyDrive/miniecoset_100obj_64px.h5"

############ LOADING ECO DATASET ############

with h5py.File(filename, "r") as f:
    # get all the keys to explore further
    print(f.keys()) 
    # extracting data
    x_train_eco = f['train']['data'][()]
    y_train_eco = f['train']['labels'][()]
    x_test_eco = f['test']['data'][()]
    y_test_eco = f['test']['labels'][()]
    
    # data = tf.data.Dataset.from_tensor_slices((data))
trainy_eco = tf.data.Dataset.from_tensor_slices((x_train_eco, y_train_eco))
testy_eco = tf.data.Dataset.from_tensor_slices((x_test_eco, y_test_eco))

x_train_eco = np.asarray([x_train_eco[index] for index in range(0, len(x_train_eco), 5)])
y_train_eco = np.asarray([y_train_eco[index] for index in range(0, len(y_train_eco), 5)])

############# PREPROCESS  NSD ################

def preprocess_NSD(x_train_data):

    # convert the 2d array output labels into 1D array
    x_train_data = x_train_data.astype('float32')

    # normalizing the training and testing data
    x_train_data /= 255.0
    x_train_data = resize_layer(x_train_data)
    x_train_data = augmentation_model(x_train_data)
    
    return x_train_data


nsd_train_images = preprocess(np.load('/content/drive/MyDrive/FinalProject/NSD_Data/nsd_train-images_subj01.npy'))

#data augmentation
#add slightly transformed/rotated copies of already included data
augmentation_model = tf.keras.Sequential([
  tf.keras.layers.RandomFlip("horizontal_and_vertical"),
  tf.keras.layers.RandomRotation(0.1),
])

# resize layer to crop eco and NSD dataset
resize_layer = tf.keras.layers.Resizing(32, 32)

def preprocess(data, batch_size, resize, data_augmentation):

  if resize:
    # Apply the Resizing layer to the datasets
    data = data.map(lambda img, target: (resize_layer(img), target))

  if data_augmentation: 
    #data augmentation
    data = data.map(lambda img, target: (augmentation_model(img), target))

  #casting image value to float32
  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))
  #sloppy input normalization, bringing image values from range [0, 255] to [-1, 1]
  data = data.map(lambda img, target: ((img/128.)-1., target))
  #creating one hot vectors for the labels
  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=100)))
  #cache this progress in memory
  #shuffle, batch, prefetch
  data = data.cache()shuffle(1000).batch(batch_size).prefetch(20)

  return data



########## GUT #######################
def visualize(images, labels):
    fig, axes = plt.subplots(3,2, figsize=(10,10))
    fig.tight_layout()

    for i, ax in enumerate(axes.flatten()):
        ax.imshow(images[i*3000])
        ax.set_title(labels)
        ax.axis("off")

visualize(x_train_eco, eco_labels[y_train_eco[i*3000]])

def visualize(images, labels):
    fig, axes = plt.subplots(3,2, figsize=(8,8))
    fig.tight_layout()

    for i, ax in enumerate(axes.flatten()):
        ax.imshow(images[i].reshape(32, 32, 3))
        ax.set_title(labels)
        ax.axis("off")

visualize(x_train_eco, 'Training image: ' + str(i+1))

## VISUALIZATION ##
def print_img(output, n=9, title = "Generated images from the generator"):
  """print n images from an array in shape(num_images,height,width)

    Args:
        output(array): output images of an generator
        n(int): number of images to be generated
        titel(string): Titel of the generated plot

  """
  fig = plt.figure()
  
  #tighten the layout for multiple images
  if n==1:
    fig.suptitle(title)
  else: fig.suptitle(title, y=1.0)

  # display first n images in output
  for i, img in enumerate(output):
    # display single image
    if i == n+1: 
      ax = plt.subplot(3, 3, i)
    else:
      ax = plt.subplot(3, 3, i+1)
    plt.imshow(img, cmap='gray_r')
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
  plt.show()

#get and show the data
for images in train_eco.take(1):
  print(images[0].shape)
  print_img(tf.squeeze(images[0]),"Glimpse on train dataset")