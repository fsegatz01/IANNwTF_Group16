# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12UBatab55UZT5AjoyFHUi4htL5pdsk-p
"""

import h5py
import tensorflow as tf

def preprocess_finally(data):
    # perform any additional preprocessing steps on the dataset as needed
    data = data.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))


    data = data.map(lambda img, target: (img, tf.one_hot(target, depth=100)))

    data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))
    #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]
    data = data.map(lambda img, target: ((img/128.)-1., target))

    #cache this progress in memory
    data = data.cache()
    data = data.shuffle(1000)
    # data = data.batch(32)
    data = data.prefetch(20)

    return data

class HDF5Sequence(tf.keras.utils.Sequence):
    def __init__(self, filename, batch_size):
        self.batch_size = batch_size
        with h5py.File(filename, 'r') as f:
            self.length = f['train']['labels'].shape[0]
        self.file = filename

    def __len__(self):
        return int(tf.math.ceil(self.length / float(self.batch_size)))

    def __gettrain__(self, idx):
        with h5py.File(self.file, 'r') as f:
            start_idx = idx * self.batch_size
            end_idx = (idx + 1) * self.batch_size
            train_img = f['train']['data'][start_idx:end_idx]
            train_labels = f['train']['labels'][start_idx:end_idx]
        return train_img, train_labels

    def __gettest__(self, idx):
        with h5py.File(self.file, 'r') as f:
            start_idx = idx * self.batch_size
            end_idx = (idx + 1) * self.batch_size
            test_img = f['test']['data'][start_idx:end_idx]
            test_labels = f['test']['labels'][start_idx:end_idx]
        return test_img, test_labels



