# -*- coding: utf-8 -*-
"""HW04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/192YJzwHw1NBiedH7dt2mxJa-6oRgsQQZ
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import tensorflow_datasets as tfds
import datetime
import numpy as np

# %load_ext tensorboard

# TASK 2.1 - Preparing MNIST math dataset
mnist = tfds.load("mnist", split =["train","test"], as_supervised=True)

# TASK 2.2 - Two MNIST math datasets
def preprocess(data, batch_size, subtask):
    # image should be float
    data = data.map(lambda x, t: (tf.cast(x, float), t))
    # image should be flattened
    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))
    # image vector will here have values between -1 and 1
    data = data.map(lambda x,t: ((x/128.)-1., t))
    # we want to have two mnist images in each example
    # this leads to a single example being ((x1,y1),(x2,y2))
    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), 
                                     data.shuffle(2000)))
    if subtask == 1:
      # map ((x1,y1),(x2,y2)) to (x1,x2, y1 + y2 >= 5)
      zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5)) # boolean
      zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32))) # turn to integer
    else:
      # map ((x1,y1),(x2,y2)) to (x1,x2, y1 - y2)
      zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1])) # integer

    # TASK 2.2 - Two MNIST math datasets
    # batch the dataset
    zipped_ds = zipped_ds.batch(batch_size)
    # prefetch
    zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)
    return zipped_ds

# TASK 3 - Building shared weight models

class TwinMNISTModel(tf.keras.Model):

    # 1. constructor
    def __init__(self):
        super().__init__()
        # inherit functionality from parent class

        # optimizer, loss function and metrics
        self.metrics_list = [tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.BinaryAccuracy(name="acc")]
        
        self.optimizer = tf.keras.optimizers.Adam()
        
        # we use the loss function and parameters we need for subtask 1 as default values
        self.loss_function = tf.keras.losses.BinaryCrossentropy()
        
        # 2 dense layers with relu as activation
        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)

        # output layer with sigmoid as activation (again activation function of subtask 1 as default)
        self.out_layer = tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)

    #getter and setter for the loss function
    def get_loss_function(self):
      return self.loss_function

    def set_loss_function(self, loss_function):
      self.loss_function = loss_function

    prop_lf = property(get_loss_function, set_loss_function)

    # getter and seter for output layer
    def get_out_layer(self):
      return self.out_layer

    def set_out_layer(self, out_layer):
      self.out_layer = out_layer

    prop_out = property(get_out_layer, set_out_layer)

    # getter and setter for the optimizer
    def get_optimizer(self):
      return self.optimizer

    def set_optimizer(self, optimizer):
      self.optimizer = optimizer

    prop_opt = property(get_optimizer, set_optimizer)

    # getter and setter for metrics list
    def get_met_list(self):
      return self.metrics_list

    def set_met_list(self, accuracy):
      self.metrics_list = accuracy

    prop_opt = property(get_met_list, set_met_list)

        
    # 2. call method (forward computation)
    def call(self, images, training=False):
        img1, img2 = images
        
        img1_x = self.dense1(img1) #image 1 as input for dense layer 1
        
        img2_x = self.dense2(img2) #image 2 as input for second dense layer
        
        combined_x = tf.concat([img1_x, img2_x ], axis=1)
        
        return self.out_layer(combined_x)


    # 3. metrics property
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model


    # 4. reset all metrics objects
    def reset_metrics(self):
        for metric in self.metrics:
            metric.reset_states()


    # 5. train step method
    def train_step(self, data):
        img1, img2, label = data
        
        with tf.GradientTape() as tape:
            output = self((img1, img2), training=True)
            label = tf.reshape(label, output.shape)
            loss = self.loss_function(label, output)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}



    # 6. test_step method
    def test_step(self, data):

        img1, img2, label = data
        output = self((img1, img2), training=False)
        label = tf.reshape(label, output.shape)
        loss = self.loss_function(label, output)

        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}

# Define where to save the log
def create_log(optimizer):

  # get the optimizer into a string to use it for the path for better recognition 
  config_name= str(optimizer)
  
  # create the path for the log file
  train_log_path = f"logs/{config_name}/train"
  val_log_path = f"logs/{config_name}/val"

  # log writer for training metrics
  train_summary_writer = tf.summary.create_file_writer(train_log_path)

  # log writer for validation metrics
  val_summary_writer = tf.summary.create_file_writer(val_log_path)

  # returning the writers for training and validation
  return train_summary_writer, val_summary_writer

# TASK 4 - Training the networks

def training(subtask, opt):

  # instantiate the model
  mymodel = TwinMNISTModel()

  # create datasets
  train_ds = mnist[0]
  val_ds = mnist[1]

  train_ds = preprocess(train_ds, batch_size=32, subtask = subtask) #train_ds.apply(preprocess)
  val_ds = preprocess(val_ds, batch_size=32, subtask = subtask) #val_ds.apply(preprocess)

  # for subtask two we use different parameters
  if subtask == 2: 
    mymodel.set_out_layer(tf.keras.layers.Dense(1,)) # set the output layer
    mymodel.set_loss_function(tf.keras.losses.MeanSquaredError()) # set a different loss function 

    mymodel.set_met_list([tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.Accuracy(name="acc")])

  mymodel.set_optimizer(opt) #set optimizer 


  # internal training loop function
  def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer, save_path): 

    #save_path = save_path

    for epoch in range(epochs):
        print(f"Epoch {epoch}:")

        # Validation:
        # (we do the validation first so that we get the accuracy and loss before training the network)
        for data in val_ds:
            metrics = model.test_step(data)
        
            # logging the validation metrics to the log file which is used by tensorboard
            with val_summary_writer.as_default():
                for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=epoch)
                    
        print([f"val_{key}: {value.numpy()}" for (key, value) in metrics.items()])

        # reset all metrics
        model.reset_metrics()    
        
        # Training:
        for data in train_ds:
            metrics = model.train_step(data)
            
            # logging the train metrics to the log file which is used by tensorboard
            with train_summary_writer.as_default(): # context Manager
                for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=epoch)

        # print the metrics
        print([f"train_{key}: {value.numpy()}" for (key, value) in metrics.items()])


        # reset all metrics
        model.reset_metrics()
        print("\n")

    #save weights
    if save_path:
        model.save_weights(save_path)
  
  train_summary_writer, val_summary_writer = create_log(opt)
  
  training_loop(mymodel, train_ds, val_ds, 10, train_summary_writer, val_summary_writer, f"logs/{str(opt)}/weights")
  return

# TASK 5 - Experiments

print("subtask 1): \n")

print("Optimizer SGD \n")
training(1, tf.keras.optimizers.SGD())

print("Optimizer Adam \n")
training(1, tf.keras.optimizers.Adam())

print("subtask 2): \n")

print("Optimizer SGD \n")
training(2, tf.keras.optimizers.SGD())

print("Optimizer Adam \n")
training(2, tf.keras.optimizers.Adam())

# OUTSTANDING

# subtask 1)
print("subtask 1) Optimizer SGD with momentum = 0.5 \n")
training(1, tf.keras.optimizers.SGD(momentum=0.5))

print("subtask 1) Optimizer Adagrad \n")
training(1, tf.keras.optimizers.Adagrad())

print("subtask 1) Optimizer RMSprop \n")
training(1, tf.keras.optimizers.RMSprop())


# subtask 2)
print("subtask 2) Optimizer SGD with momentum = 0.5 \n")
training(2, tf.keras.optimizers.SGD(momentum=0.5))

print("subtask 2) Optimizer Adagrad \n")
training(2, tf.keras.optimizers.Adagrad())

print("subtask 2) Optimizer RMSprop \n")
training(2, tf.keras.optimizers.RMSprop())

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/