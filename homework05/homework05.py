# -*- coding: utf-8 -*-
"""homework05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1du7fYOQpsgKyiur5es0g5UHliXWQQI__
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
# %load_ext tensorboard

cifar10 = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)

def preprocess_data(cifar10):
  #convert data from uint8 to float32
  cifar10 = cifar10.map(lambda img, target: (tf.cast(img, tf.float32), target))
  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]
  cifar10 = cifar10.map(lambda img, target: ((img/128.)-1., target))
  #create one-hot targets
  cifar10 = cifar10.map(lambda img, target: (img, tf.one_hot(target, depth=10)))
  #cache this progress in memory, as there is no need to redo it; it is deterministic after all
  cifar10 = cifar10.cache()
  #shuffle, batch, prefetch
  cifar10 = cifar10.shuffle(1000)
  cifar10 = cifar10.batch(32)
  cifar10 = cifar10.prefetch(20)
  #return preprocessed dataset
  return cifar10

def try_model(model, ds):
  for x, t in ds.take(5):
    y = model(x)

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Reduce pixel values
x_train, x_test = x_train / 255.0, x_test / 255.0
 
# flatten the label values
y_train, y_test = y_train.flatten(), y_test.flatten()

# these are the labels for the cifar10 data set, so 0 means airplane, 1 means automobile and so on
labels = '''airplane automobile bird cat deer dog frog horse ship truck'''.split()

fig, axes = plt.subplots(3,3, figsize=(12,12))
fig.tight_layout()

for i, ax in enumerate(axes.flatten()):
    ax.imshow(x_train[i].reshape(32,32,3))
    ax.set_title(labels[y_train[i]])
    ax.axis("off")

from tensorflow.python.framework.importer import import_graph_def_for_function
class BasicConv(tf.keras.Model):
    def __init__(self):
        super(BasicConv, self).__init__()

        self.convlayer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')
        self.convlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')
        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)

        self.convlayer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')
        self.convlayer4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')
        self.global_pool = tf.keras.layers.GlobalAvgPool2D()

        self.out = tf.keras.layers.Dense(10, activation='softmax')

        # optimizer, loss function and metrics
        self.metrics_list = [tf.keras.metrics.Mean(name="loss"),
                             tf.keras.metrics.BinaryAccuracy(name="acc")]
        

        self.loss_function = tf.keras.losses.CategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.005) # optimizer SGD

    def call(self, x):
        x = self.convlayer1(x)
        x = self.convlayer2(x)
        x = self.pooling(x)
        x = self.convlayer3(x)
        x = self.convlayer4(x)
        x = self.global_pool(x)
        x = self.out(x)
        return x
    
     # 3. metrics property
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model



    # 4. reset all metrics objects
    def reset_metrics(self):
        for metric in self.metrics:
            metric.reset_states()

    def get_optimizer(self):
      return self.optimizer

    def set_optimizer(self, optimizer):
      self.optimizer = optimizer

    prop_opt = property(get_optimizer, set_optimizer)


    # 5. train step method
    def train_step(self, data):
        img, label = data
        
        with tf.GradientTape() as tape:
            output = self(img, training=True)
            label = tf.reshape(label, output.shape)
            loss = self.loss_function(label, output)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}



    # 6. test_step method
    def test_step(self, data):

        img, label = data
        output = self(img, training=False)
        label = tf.reshape(label, output.shape)
        loss = self.loss_function(label, output)

        # update the state of the metrics according to loss and accuracy
        self.metrics[0].update_state(loss)
        self.metrics[1].update_state(label, output)

        # return a dictionary with metric names as keys and metric results as values
        return {m.name: m.result() for m in self.metrics}

''' HYPERPARAMETERS
CrossEntropyLoss()
learning_rate = 0.005
Epoch = maximal 15, maybe 11
batch_size = 10?
SGD optimizer?
'''

# Define where to save the log
def create_log(optimizer):

  # get the optimizer into a string to use it for the path for better recognition 
  config_name= str(optimizer)
  
  # create the path for the log file
  train_log_path = f"logs/{config_name}/train"
  test_log_path = f"logs/{config_name}/test"

  # log writer for training metrics
  train_summary_writer = tf.summary.create_file_writer(train_log_path)

  # log writer for validation metrics
  test_summary_writer = tf.summary.create_file_writer(test_log_path)

  # returning the writers for training and validation
  return train_summary_writer, test_summary_writer

# TASK 4 - Training the networks

def training(opt):
  
  # instantiate the model
  mymodel = BasicConv()

  # create datasets
  train_ds = cifar10[0]
  test_ds = cifar10[1]

  train_ds = preprocess_data(train_ds) #train_ds.apply(preprocess)
  test_ds = preprocess_data(test_ds) #val_ds.apply(preprocess)
  

  mymodel.set_optimizer(opt) #set optimizer 


  # internal training loop function
  def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path): 

    #save_path = save_path

    for epoch in range(epochs):
        print(f"Epoch {epoch}:")

        # Validation:
        # (we do the validation first so that we get the accuracy and loss before training the network)
        for data in test_ds:
            metrics = model.test_step(data)
        
            # logging the validation metrics to the log file which is used by tensorboard
            with test_summary_writer.as_default():
                for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=epoch)
                    
        print([f"test_{key}: {value.numpy()}" for (key, value) in metrics.items()])

        # reset all metrics
        model.reset_metrics()    
        
        # Training:
        for data in train_ds:
            metrics = model.train_step(data)
            
            # logging the train metrics to the log file which is used by tensorboard
            with train_summary_writer.as_default(): # context Manager
                for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=epoch)

        # print the metrics
        print([f"train_{key}: {value.numpy()}" for (key, value) in metrics.items()])


        # reset all metrics
        model.reset_metrics()
        print("\n")

    #save weights
    if save_path:
        model.save_weights(save_path)
  
  train_summary_writer, test_summary_writer = create_log(opt)
  
  training_loop(mymodel, train_ds, test_ds, 11, train_summary_writer, test_summary_writer, f"logs/{str(opt)}/weights")
  return

training(tf.keras.optimizers.SGD(momentum=0.5))



# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/